{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuydOXTs7w4o"
      },
      "source": [
        "# Vectorial Word Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjOQt-Rgelp"
      },
      "source": [
        "## Background\n",
        "Representing words as dense vectors over a finite-dimensional space was one of the recent breakthroughs in Natural Language Processing. Vectorial representations allow space-efficient, informationally rich storage of words that adequately captures their semantic content and enables numerical computation on them. Word vectors are the standard input representation for machine learning architectures for language processing. Even though new methods for constructing such representations emerge frequently, the original set of published papers remain a de facto point of reference as well as a good starting point. For this assignment, you will be asked to implement a small-scale variant of one such paper, namely [Global Word Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (\"the GloVe paper\").\n",
        "\n",
        "Notes on the paper will appear throughout the notebook to guide you along the code. It is, however, important to read and understand the paper, its terminology and the theory behind it before attempting to go through with the assignment. Some of the tasks will also require addressing the paper directly.\n",
        "\n",
        "---\n",
        "\n",
        "There are 2 types of tasks in this assignment:\n",
        "- coding tasks --- asking you to write code following specifications provided; Most of the tasks come with test cases for sanity-check. Still, if something is not clear, <ins>do ask questions to lab teachers</ins>. When working with tensors, you have to use tensor-related operations instead of, e.g., inefficiently looping through tensor rows and columns.\n",
        "- interpretation questions --- asking you to interpret the data or the results of the model\n",
        "\n",
        "Each comes with its predefined points (totaling to 50pt). Some coding tasks have 0 points but solving them will be useful for you.\n",
        "\n",
        "---\n",
        "\n",
        "You are greatly encouraged to add comments to your code describing what particular lines of code do (in general, a great habit to have in your coding life).\n",
        "Additionally please follow these rules when submitting the notebook:\n",
        "\n",
        "* Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
        "* For theoretical questions, put your solution in the `YOUR ANSWER HERE` cell and keep the header(!).\n",
        "* Don't change or delete any initially provided cells, either text or code, unless explicitly instructed to do so.\n",
        "* Don't delete the comment lines `# TEST...` or edit their code cells. The test cells are for sanity checking. Passing them doesn't necessarily means that your code is fine.\n",
        "* Don't change the names of provided functions and variables or arguments of the functions.\n",
        "* Don't clear the output of your code cells.\n",
        "* Don't output unnecessary info (e.g., printing variables for debugging purposes). This clutters the notebook and slows down the grading. You can have print() in the code, but comment them out before submitting the notebook.\n",
        "* Delete those cells that you inserted for your own debuging/testing purposes.\n",
        "* Don't forget to fill in the contribution information.\n",
        "* Test your code and **make sure we can run your notebook** in the colab environment.\n",
        "* A single notebook file (without archiving) per group should be submitted via BB.\n",
        "\n",
        "<font color=\"red\">You following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNT1WNEnlkBC"
      },
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "Names of Group 64:\n",
        "* Youssef Ben mansour\n",
        "* Tim Kramer\n",
        "\n",
        "Contributions:\n",
        "* We both contributed equally to each exercise. We worked on them independently at first, then compared the results and improved when necessary. However, while Youssef focussed a bit more on completing c8, Tim focussed a bit more on rewriting some of the i-questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyHEls6Agelp"
      },
      "source": [
        "## Corpus Statistics\n",
        "\n",
        "The paper's proposed model, GloVe, aims to densely represent words in a way that captures the global corpus statistics.\n",
        "\n",
        "The construction it encodes is the word __co-occurrence matrix__. A co-occurrence matrix is a simplistic data structure that counts the number of times each word has appeared within the context of every other word. The definition of a context varies; usually, context is implied to be a fixed-length span (that may or may not be allowed to escape sentence boundaries) around a word.\n",
        "\n",
        "For instance, in the sentence below and for a context length of 2, the word <span style=\"color:pink\">__Earth__</span> occurs in the context of <span style=\"color:lightgreen\">made</span> (1), <span style=\"color:lightgreen\">on</span> (1), <span style=\"color:lightgreen\">as</span> (1), <span style=\"color:lightgreen\">an</span> (1).\n",
        "\n",
        "> \"He struck most of the friends he had <span style=\"color:lightgreen\">made on</span> <span style=\"color:pink\">__Earth__</span> <span style=\"color:lightgreen\">as an</span> eccentric\"\n",
        "\n",
        "Similarly, the word <span style=\"color:pink\">__friends__</span> occurs in the context of <span style=\"color:lightgreen\">of</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">he</span> (1), <span style=\"color:lightgreen\">had</span> (1).\n",
        "\n",
        "> \"He struck most <span style=\"color:lightgreen\">of the</span> <span style=\"color:pink\">__friends__</span> <span style=\"color:lightgreen\">he had</span> made on Earth as an eccentric\"\n",
        "\n",
        "An alternative definition of a context would be, for instance, the variable-length windows spanned by a full sentence.\n",
        "\n",
        "Contexts may be summed across sentences or entire corpora; the summed context of <span style=\"color:pink\">__he__</span> in the example sentence is: <span style=\"color:lightgreen\">struck</span> (1), <span style=\"color:lightgreen\">most</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">friends</span> (1), <span style=\"color:lightgreen\">had</span> (1), <span style=\"color:lightgreen\">made</span> (1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcPQmxFagelp"
      },
      "source": [
        "For the purposes of this assignment, we have prepared a co-occurrence matrix over a minimally processed version of the Harry Potter books.\n",
        "\n",
        "(A few interpretation tasks in this assignment presuppose some minimal level of familiarity with the Harry Potter books/films. If no one in your group is familiar with Harry Potter, you might find the [fandom page](https://harrypotter.fandom.com/wiki/Main_Page) useful or the [synopsis sections](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Philosopher%27s_Stone) of the corresponding wiki pages.\n",
        "\n",
        "The pickle file contains three items:\n",
        "1. `vocab`: a dictionary mapping words to unique ids, containing $N$ unique words\n",
        "2. `contexts`: a dictionary mapping words to their contexts, where contexts are themselves dicts from words to integers that show the number of co-occurrences between these words.\n",
        "    E.g. `{\"portrait\": {\"harry\": 124, \"said\": 114, ...}, ...}` meaning that the word \"harry\" has appeared in the context of the word \"portrait\" 124 times, etc.\n",
        "3. `X`: a torch LongTensor ${X}$ of size $N \\times N$, where ${X}[i,j]$ denotes the number of times the word with id $j$ has appeared in the context of the word with id $i$\n",
        "\n",
        "Extremely common or uncommon words (i.e. words with too few or too many global occurrences) have been filtered out for practical reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4xXGmBsgelp"
      },
      "outputs": [],
      "source": [
        "import pickle, math\n",
        "import torch\n",
        "from torch import FloatTensor, LongTensor\n",
        "from typing import Dict, Callable, List\n",
        "# torch.set_printoptions(precision=8) #to increase precision of printing floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8FXEspuud42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70b3515-dcd9-426b-da1b-3bff47352478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-24 11:22:22 URL:https://naturallogic.pro/_files_/download/MLHVL/HP-Counts.p [173580603/173580603] -> \"HP-Counts.p\" [1]\n"
          ]
        }
      ],
      "source": [
        "# this command downloads the pickle file.\n",
        "# This is not a python code, it is a unix code. You can run system commands in jupyter notebooks.\n",
        "# !wget -nv -O HP-Counts.p https://www.dropbox.com/scl/fi/dnm7s38j8d0k0bguisiby/HP-Counts.p?rlkey=j0fc11rlnkow7jqb02sel6gz6&dl=1\n",
        "# try this if download from the dropbox link doesn't work\n",
        "!wget -nv -O HP-Counts.p https://naturallogic.pro/_files_/download/MLHVL/HP-Counts.p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvW_GgYDgelr",
        "nbgrader": {
          "grade": false,
          "grade_id": "file-open",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "with open(\"HP-Counts.p\", \"rb\") as f:\n",
        "    vocab, contexts, X = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYhg1c9gels"
      },
      "source": [
        "Let's inspect the top 10 most frequent words in the context of the word 'portrait'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Z-eEFcgelt",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69f29bb-28b9-4a9f-93ad-5dcf668b8c31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('harry', 124),\n",
              " ('said', 114),\n",
              " ('hole', 85),\n",
              " ('ron', 57),\n",
              " ('hermione', 54),\n",
              " ('room', 48),\n",
              " ('fat', 45),\n",
              " ('lady', 43),\n",
              " ('common', 37),\n",
              " ('back', 31)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "sorted([(item, value) for item, value in contexts[\"portrait\"].items()], key=lambda x: x[1], reverse=True)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTRNE9o7gelt"
      },
      "source": [
        "The co-occurrence matrix of a very large corpus should give a meaningful summary of how a word is used in general. A single row of that matrix is already a __word vector__ of size $N$. However such vectors are extremely sparse, and for large corpora the size of $N$ will become unwieldy. We will follow the paper in designing an algorithm that can compress the word vectors while retaining most of their informational content.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "For the resulting vectors to actually be informative, the source corpus should have a size of at least a few billion words; on the contrary, our corpus enumerates merely a million words, so we can't expect our results to be as great.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd45Trplgelu"
      },
      "source": [
        "### From co-occurrence to probabilities\n",
        "\n",
        "Our matrix $X$ is very sparse; most of its elements are zero.\n",
        "\n",
        "Find what the ratio of non-zero elements is.  \n",
        "Check if the matrix is symmetric (think about why it should (not) be).\n",
        "\n",
        "_Hint_: The function `non_zero_ratio` should return a `float` rather than a `FloatTensor`. Remember `.item()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVb5uBzngelu",
        "nbgrader": {
          "grade": false,
          "grade_id": "non_zero_ratio",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c1.1 [0pt]\n",
        "def non_zero_ratio(sparse_matrix: LongTensor) -> float:\n",
        "    num_non_zeros =(sparse_matrix != 0).sum().item()\n",
        "    i,j = sparse_matrix.shape\n",
        "    total = i*j\n",
        "    return num_non_zeros/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqLY3Kmugelu",
        "nbgrader": {
          "grade": true,
          "grade_id": "non_zero_ratio_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c1.1\n",
        "assert 0.1 < non_zero_ratio(X) < 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJsqVzO_gelu"
      },
      "source": [
        "We will soon need to perform division and find the logarithm of ${X}$. Neither of the two operations are well-defined for $0$. That's why for further processing we want to have a matrix without any zero elements.\n",
        "\n",
        "Change the matrix's datatype to a `torch.float` and add a small constant to it such as $0.1$ to ensure numerical stability while maintaining sparsity. The obtained matrix will be used in the remaining sections (not the original one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atv-IK8dgelv",
        "nbgrader": {
          "grade": false,
          "grade_id": "X1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c1.2 [0pt]\n",
        "X1 =  X.to(torch.float) + 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OVZ556bgelv",
        "nbgrader": {
          "grade": true,
          "grade_id": "X1_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c1.2\n",
        "assert non_zero_ratio(X1) == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ZXji04gelw"
      },
      "source": [
        "From the paper:\n",
        "> Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.  Let $X_i$= $\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij} = P(j  | i) =  X_{ij}/X_i$ be the probability that word $j$ appears in the context of word $i$.\n",
        "\n",
        "Complete the function `to_probabilities` that accepts a co-occurrence matrix and returns the probability matrix $P$. Use tensor-specific functions and methods hiel doing this (e.g., don't loop through colums and rows of the tensor).\n",
        "\n",
        "_Hint_: Remember broadcasting and `torch.sum()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZQ0Upzogelw",
        "nbgrader": {
          "grade": false,
          "grade_id": "to_probabilities",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c1.3 [1pt]\n",
        "def to_probabilities(count_matrix: FloatTensor) -> FloatTensor:\n",
        "    result = count_matrix/torch.sum(count_matrix, dim=1)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFs3I2hLgelw",
        "nbgrader": {
          "grade": false,
          "grade_id": "to_probabilities_run",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "P = to_probabilities(X1) # note that we use X1 not X here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1ulUiEjgelw",
        "nbgrader": {
          "grade": true,
          "grade_id": "to_probabilities_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c1.3\n",
        "assert P.shape == torch.Size([len(vocab), len(vocab)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZwxb3Mgelx"
      },
      "source": [
        "### Probing words\n",
        "\n",
        "From the paper:\n",
        "> Consider two words $i$ and $j$ that exhibit a particular aspect of interest. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$.  For words $k$ related to $i$ but not $j$, we expect the ratio $P_{ik}/P_{jk}$ will be large.  Similarly, for words $k$ related to $j$ but not $i$, the ratio should be small. For words $k$ that are either related to both $i$ and $j$, or to neither, the ratio should be close to one.\n",
        "\n",
        "Complete the function `query` that accepts two words $w_i$ and $w_j$, a vocab $V$ and a probability matrix ${P}$, maps each word to its corresponding index and returns the probability $P(j  |  i)$. If such probability is impossible to compute for input words, return float 0. probability.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBdjawQGgelx",
        "nbgrader": {
          "grade": false,
          "grade_id": "query",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c2.1 [1pt]\n",
        "def query(word_i: str, word_j: str, vocab: Dict[str, int], prob_matrix: FloatTensor) -> float:\n",
        "    i = vocab[word_i]\n",
        "    j = vocab[word_j]\n",
        "    return prob_matrix[j,i].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSnC-kk-gelx",
        "nbgrader": {
          "grade": true,
          "grade_id": "query_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c2.1\n",
        "assert round(query('harry', 'potter', vocab, P), 5) == 0.00353"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlVGaJVbgelx"
      },
      "source": [
        "Then, complete the function `probe` that accepts three words $w_i$, $w_j$ and $w_k$, a vocab $V$ and a probability matrix ${P}$, calls `query` and returns the ratio $P(k |  i) / P(k  |  j)$. Let the function return 0 in case of division by 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9wW_9WLgelx",
        "nbgrader": {
          "grade": false,
          "grade_id": "probe",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c2.2 [1pt]\n",
        "def probe(word_i: str, word_j: str, word_k: str, vocab: Dict[str, int], prob_matrix: FloatTensor) -> float:\n",
        "    p1 = query(word_i,word_k,vocab,prob_matrix)\n",
        "    p2 = query(word_j,word_k,vocab,prob_matrix)\n",
        "    if p2 == 0:\n",
        "        return 0\n",
        "    return p1/p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1_riqA8gely",
        "nbgrader": {
          "grade": true,
          "grade_id": "probe_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c2.2\n",
        "assert round(probe('harry', 'potter', 'stone', vocab, P), 4) == 1.3872"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "491Qc7WJgely"
      },
      "source": [
        "Let's probe a few words and examine whether the authors' claim holds even for our (tiny) corpus. **Add two pairs of your own word triplets** and experiment on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XVgjB7lgely",
        "nbgrader": {
          "grade": true,
          "grade_id": "probing",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"tea\", \"wand\", \"magic\", probe(\"tea\", \"wand\", \"magic\", vocab, P))\n",
        "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\n",
        "print()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"avada\",\"happy\",\"kedavra\",probe(\"avada\",\"happy\",\"kedavra\",vocab,P))\n",
        "print(\"hogwarts\",\"diagon\",\"galleons\",probe(\"hogwarts\",\"diagon\",\"galleons\",vocab,P))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGDY0xucgelz"
      },
      "source": [
        "#### i1 [1pt]\n",
        "Give a brief interpretation of the results you got. Do they correspond to your expectations? Why or why not?\n",
        "\n",
        "*Hint*: When do we expect the ratio value to be high, low or close to 1? Refer to the GloVe paper for guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCC40-wBgelz",
        "nbgrader": {
          "grade": true,
          "grade_id": "interpretation1",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "The results we got meet our expectations, as they reflect relations between words that are commonly associated with each other in the Harry Potter world.\n",
        "\n",
        "Our first example, the probe function returns the ratio P(kedavra|avada)/P(kedavra|happy). This ratio value is remarkably high, as the probability of kedavra given avada is probably much higher than kedavra given happy. In the world of Harry Potter, the combination of the words ‘avada kedavra’ is a famous killing curse, making it quite obvious that both words follow each other and have a ratio much higher than many other combinations with one of the words.\n",
        "\n",
        "In the second example, P(galleons|hogwarts)/P(galleons|diagon) we see a much lower ratio value. Galleons are the currency in the world of Harry Potter and Diagon Alley is a street with many storefronts. Therefore, we expect lots of mentions of galleons around ‘diagon’, much more than around ‘hogwarts’, which is the school of Harry Potter, and we expect less financial transactions to be made. Therefore, the denominator of this probe ratio is much higher than its numerator, resulting in a lower combined probe ratio.\n",
        "\n",
        "According to the GloVe paper, when the ratio is high k is more strongly related to i than j. When the ratio is low, the opposite. When the ratio is around 1, this could be because a word is either related (with a similar ratio) to both words, or to neither **(page 3 of GloVe pdf)**.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNpZ5cpUgelz"
      },
      "source": [
        "What would happen if we tried probing out-of-domain words? Use the words \"solid\", \"gas\", \"water\", and \"fashion\", which the authors report in the paper in the context of \"ice\" and \"steam\" (Table 1). Make your code to clearly print the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2mZvmINgelz",
        "nbgrader": {
          "grade": true,
          "grade_id": "ice_steam",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8070227b-4d76-46df-bfce-27956d0bb615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ice steam solid 0.07830312620153405\n",
            "ice steam gas 0.8613343808912856\n",
            "ice steam water 1.27149363325054\n",
            "ice steam fashion 0.8613343808912856\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "print(\"ice\", \"steam\", \"solid\", probe(\"ice\", \"steam\", \"solid\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"gas\", probe(\"ice\", \"steam\", \"gas\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"water\", probe(\"ice\", \"steam\", \"water\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"fashion\", probe(\"ice\", \"steam\", \"fashion\", vocab, P))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydVy5ca4gelz"
      },
      "source": [
        "#### i2 [3pt]\n",
        "Give an interpretation of the results you got. Do they match what the authors report in the paper? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-WHvhh1gel0",
        "nbgrader": {
          "grade": true,
          "grade_id": "interpretation2",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "The result differs from the paper's because we used a different vocabulary to construct the probability matrix. These vocabularies have different word frequencies and context occurrences, leading to variations in the probability values\n",
        "P(k∣i)/P(j|k), which results in different outcomes. In the GloVe paper, the authors used a 6 billion token corpus, likely covering more of the physical properties of water than the Harry Potter corpus.\n",
        "\n",
        "The ratio for ‘ice’, ‘steam’, ‘water’ scores around 1 (score of around 1.27). This meets our expectations, as steam and ice are both forms that water can take. Similar reasoning can be used to explain the relatively close to 1 score (around 0.86) of 'ice', 'steam', 'gas' - all three forms that water can take. The models accuracy to capture these relations is  also confirmed by the lower score of ‘ice’, ‘steam’, ‘solid’ (around 0.08), suggesting a weaker relationship between solid and steam than solid and ice.\n",
        "\n",
        "However, the ratio for ‘ice’, ‘steam’, ‘fashion’ also scores remarkably close to 1 (around 0.86). This is unexpected, as fashion and steam do not seem to have a strong relationship. This indicates that the model captures some expected relationships when dealing with out-of-domain words, and it may produce less reliable associations for words that are not properly presented in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4tjPd5Qgel0"
      },
      "source": [
        "## Dense Vectors\n",
        "\n",
        "Now, we would like to convert these long sparse vectors into short dense ones.\n",
        "\n",
        "The conversion should be such that the probability ratios we inspected earlier may still be reconstructed via some (for now, unknown) operation $F$ on the dense vectors.\n",
        "\n",
        "To restrict the search space over potential functions, the authors impose a number of constraints they think $F$ should satisfy:\n",
        "1. > While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. $F$ should be dot-product based.\n",
        "2. > The distinction between a word and a context word is arbitrary and we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$ but also $X \\leftrightarrow X^T$.\n",
        "3. > It should be well-defined for all values in $X$.\n",
        "\n",
        "Given these three constraints, each word $i$ in our vocabulary is represented by four vectors:\n",
        "1. A vector $w_i \\in \\mathbb{R}^D$\n",
        "2. A bias $b_i \\in \\mathbb{R}$\n",
        "3. A context vector $\\tilde{w}_i \\in \\mathbb{R}^D$\n",
        "4. A context bias $\\tilde{b}_i \\in \\mathbb{R}$\n",
        "\n",
        "and $F: \\mathbb{R}^D \\times \\mathbb{R} \\times \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}$ is defined as:\n",
        "\n",
        "$F(w_i, \\tilde{w}_k, b_i, \\tilde{b}_k) = w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k$.\n",
        "\n",
        "Or equivalently the least squares error $J$ is minimized, where:\n",
        "\n",
        "$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$\n",
        "\n",
        "with $f$ being a weighting function, defined as\n",
        "\n",
        "$f: \\mathbb{R} \\to \\mathbb{R} = \\begin{cases}\n",
        "    (x/x_{max})^\\alpha, & \\text{if $x<x_{max}$}\\\\\n",
        "    1, & \\text{otherwise}.\n",
        "  \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k0pPj8cgel0"
      },
      "source": [
        "### Weighting Function\n",
        "\n",
        "Let's start with the last part.\n",
        "\n",
        "Complete the weighting function `weight_fn` which accepts a co-occurrence matrix ${X}$, a maximum value $x_{max}$ and a fractional power $\\alpha$, and returns the weighted co-occurrence matrix $f({X})$.\n",
        "\n",
        "Then, compute $\\text{X_weighted}$, the matrix ${X}$ after weighting, using the paper's suggested parameters.\n",
        "\n",
        "_Hint_: Note that $f$ is defined pointwise, so our weighting function should also be pointwise. Use tensor-specific functions/methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT840aDLgel0",
        "nbgrader": {
          "grade": false,
          "grade_id": "weight_fn",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c3 [1pt]\n",
        "def weight_fn(X: FloatTensor, x_max: int, alpha: float) -> FloatTensor:\n",
        "    result = torch.where(X < x_max, torch.pow((X/x_max), alpha), 1)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIkY4abRgel0",
        "nbgrader": {
          "grade": false,
          "grade_id": "X_weighted",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "X_weighted = weight_fn(X1, x_max=100, alpha=3/4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuQFRuWrgel0",
        "nbgrader": {
          "grade": true,
          "grade_id": "weight_fn_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c3\n",
        "assert X_weighted.shape == X1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1c6SOjTgel1"
      },
      "source": [
        "Try to get an understanding of how the weighting affects different co-occurrence values (high and low). Think of some word pairs with high and low co-occurrence and look them up in $X$ and in $\\text{X_weighted}$ to get a better idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vImSYqq1gel1",
        "nbgrader": {
          "grade": true,
          "grade_id": "loss_sandbox",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "i = vocab[\"magic\"]\n",
        "j = vocab[\"wand\"]\n",
        "print(X[i,j])\n",
        "print(X_weighted[i,j])\n",
        "\n",
        "print()\n",
        "\n",
        "i = vocab[\"magic\"]\n",
        "j = vocab[\"cup\"]\n",
        "print(X[i,j])\n",
        "print(X_weighted[i,j])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGc4Dp0agel2"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "The next step is to write the loss function.\n",
        "\n",
        "We can write it as a pointwise function, apply it iteratively over each pair of words and then sum the result; that's however extremely inefficient.\n",
        "\n",
        "Inspecting the formulation of $J$, it is fairly straightforward to see that it can be immediately implemented using matrix-matrix operations, as:\n",
        "\n",
        "$J = \\sum_{i,j=1}^{V}f(\\mathbf{X})\\cdot(W\\tilde{W}^T + b + \\tilde{b}^T - log(X))^2$,\n",
        "\n",
        "where $W$, $\\tilde{W}$ are the $N \\times D$ matrices containing the $D$-dimensional vectors of all our $N$ vocabulary words, and $b$, $\\tilde{b}$ are the $N \\times 1$ matrices containing the $1$-dimensional biases of our words.\n",
        "\n",
        "Complete `loss_fn`, a function that accepts a weighted co-occurrence matrix $f({X})$, the word vectors and biases $W$, $\\tilde{W}$, $b$, $\\tilde{b}$ and the co-occurrence matrix ${X}$, and computes $J$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA4W3vXjgel2",
        "nbgrader": {
          "grade": false,
          "grade_id": "loss_fn",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c4 [1pt]\n",
        "def loss_fn(\n",
        "    X_weighted: FloatTensor,\n",
        "    W: FloatTensor,\n",
        "    W_context: FloatTensor,\n",
        "    B: FloatTensor,\n",
        "    B_context: FloatTensor,\n",
        "    X: FloatTensor\n",
        ") -> FloatTensor: # we need a tensor type as an output to be able to do backward propagation with it\n",
        "    result = torch.sum(X_weighted * torch.pow(torch.matmul(W, W_context.T) + B + B_context.T - torch.log(X), 2))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wti3lircgel2"
      },
      "source": [
        "Let's make sure that we are on a right track. For this we calculate the loss function with toy input: matrices are of size $2 \\times 2$ while bias vectors of size $2 \\times 1$. You can verify the answer manually and with your implementation of `loss_fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOPO7NLmgel2",
        "nbgrader": {
          "grade": true,
          "grade_id": "loss_fn_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c4\n",
        "toy_X_weighted = torch.FloatTensor([[.5,1],[.2,.1]])\n",
        "toy_X1 = torch.FloatTensor([[2,1],[1,5]])\n",
        "toy_W1 = torch.FloatTensor([[1,2],[1,0]]) # for W\n",
        "toy_W2 = torch.FloatTensor([[0,1],[1,2]]) # for W~\n",
        "toy_b1 = torch.FloatTensor([[0],[2]]) # for b\n",
        "toy_b2 = torch.FloatTensor([[2],[1]]) # for b~\n",
        "# fill the correct value\n",
        "assert loss_fn(toy_X_weighted, toy_W1, toy_W2, toy_b1, toy_b2, toy_X1).isclose(FloatTensor([45.2391]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuW6mKUygel2"
      },
      "source": [
        "### GloVe\n",
        "\n",
        "We have the normalized co-occurrence matrix ${X}$, the weighting function $f$, and the loss function $J$ that implements $F$.\n",
        "\n",
        "What we need now is a mapping from words (or word ids) to unique, parametric and trainable vectors.\n",
        "\n",
        "Torch provides this abstraction in the form of [Embedding layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding). Each such layer may be viewed as a stand-alone network that can be optimized using the standard procedure we have already seen. It is recommended to read about Embedding class. In general, it does the same job as `nn.Parameter(torch.rand(...))` but additionally allows to select row vectors by giving a list of token indices.   \n",
        "\n",
        "We will utilize the `nn.Module` class to contain all our embedding layers and streamline their joint optimization.\n",
        "The container class will be responsible for a few things:\n",
        "\n",
        "1. Wrapping the embedding layers:\n",
        "    1. A vector embedding that maps words to $w \\in \\mathbb{R}^D$\n",
        "    2. A context vector embedding that maps words to $w_c \\in \\mathbb{R}^D$\n",
        "    3. A bias embedding that maps words to $b \\in \\mathbb{R}^1$\n",
        "    4. A context bias embedding that maps words to $b_c \\in \\mathbb{R}^1$\n",
        "2. Implementing `forward`, a function that accepts a weighted co-occurrence matrix $f(X)$, the co-occurrence matrix $X$, then finds the embeddings of all words and finally calls `loss_fn` as defined above.\n",
        "3. Implementing `get_vectors`, a function that receives no input and produces the word vectors and context word vectors of all words, adds them together and returns the result, in accordance with the paper:\n",
        "> ...With this in mind, we choose to use the sum $W + \\tilde{W}$ as our word vectors.\n",
        "\n",
        "Complete the network class following the above specifications.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpMpVqe-gel2",
        "nbgrader": {
          "grade": true,
          "grade_id": "GloVe",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c5 [4pt]\n",
        "class GloVe(torch.nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, int], vector_dim: int=30, device: str=\"cpu\", seed: int=0) -> None:\n",
        "        super(GloVe, self).__init__()\n",
        "        self.device = device\n",
        "        self.vocab_len = len(vocab)\n",
        "        torch.manual_seed(seed) #random initialization of w, wc, b, bc is fixed by the seed\n",
        "        self.w = torch.nn.Embedding(num_embeddings=self.vocab_len, embedding_dim=vector_dim)\n",
        "        self.wc = torch.nn.Embedding(num_embeddings=self.vocab_len, embedding_dim=vector_dim)\n",
        "        self.b = torch.nn.Embedding(num_embeddings=self.vocab_len, embedding_dim=1)\n",
        "        self.bc = torch.nn.Embedding(num_embeddings=self.vocab_len, embedding_dim=1)\n",
        "\n",
        "    def forward(self, X_weighted: FloatTensor, X: FloatTensor) -> FloatTensor:\n",
        "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
        "        W = self.w(embedding_input)\n",
        "        return loss_fn(X_weighted,W,self.wc(embedding_input),self.b(embedding_input),self.bc(embedding_input),X)\n",
        "\n",
        "    def get_vectors(self) -> FloatTensor:\n",
        "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
        "        return self.w(embedding_input) + self.wc(embedding_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWb-G60hXXcw"
      },
      "outputs": [],
      "source": [
        "# TEST c5\n",
        "assert GloVe(vocab, vector_dim=30, seed=0).w.num_embeddings == len(vocab)\n",
        "assert GloVe(vocab, vector_dim=30, seed=0).bc.num_embeddings == len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHNF2C8Jgel3"
      },
      "source": [
        "### Training\n",
        "\n",
        "Everything is in place; now we may begin optimizing our embedding layers (and in doing so, the vectors they assign).\n",
        "\n",
        "Instantiate the network class you just defined using $D = 30$. Then instantiate an `Adam` optimizer with a learning rate of 0.05 and train your network for 300 epochs (don't change the default seed value).\n",
        "\n",
        "When writing the training script, remember that your network's forward pass is __already__ computing the loss. Make sure to **print a loss value for each epoch**.\n",
        "\n",
        "Training won't take too long on a CPU. In case you want to use a GPU, make sure that variables are correctly moved to a GPU with a `device` argument of class `GloVe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U-QMxyngel3",
        "nbgrader": {
          "grade": true,
          "grade_id": "training",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c6.1 [4pt]\n",
        "def train_glove_vectors(voc: Dict[str, int], X_weighted: FloatTensor, X1:FloatTensor,\n",
        "                        vector_dim: int=30, seed: int=0, lr: float=0.05, num_epochs: int=300):\n",
        "    network = GloVe(vocab, vector_dim)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr)\n",
        "\n",
        "    losses = [] # collect losses for each epoch here\n",
        "    for i in range(num_epochs):\n",
        "        loss = network.forward(X_weighted, X1)\n",
        "        losses.append(loss.item())\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        print(f\"Epoch {i:>3}: training Loss: {loss}\")\n",
        "    return losses, network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pYdq9fK4RS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0851c330-5ec7-43e3-b1bc-b5fe57720561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0: training Loss: 11233914.0\n",
            "Epoch   1: training Loss: 9747190.0\n",
            "Epoch   2: training Loss: 8462501.0\n",
            "Epoch   3: training Loss: 7357555.5\n",
            "Epoch   4: training Loss: 6410067.0\n",
            "Epoch   5: training Loss: 5599318.5\n",
            "Epoch   6: training Loss: 4906458.0\n",
            "Epoch   7: training Loss: 4314543.5\n",
            "Epoch   8: training Loss: 3808523.5\n",
            "Epoch   9: training Loss: 3375189.0\n",
            "Epoch  10: training Loss: 3003075.5\n",
            "Epoch  11: training Loss: 2682324.0\n",
            "Epoch  12: training Loss: 2404525.75\n",
            "Epoch  13: training Loss: 2162565.0\n",
            "Epoch  14: training Loss: 1950476.625\n",
            "Epoch  15: training Loss: 1763321.75\n",
            "Epoch  16: training Loss: 1597080.75\n",
            "Epoch  17: training Loss: 1448565.0\n",
            "Epoch  18: training Loss: 1315332.125\n",
            "Epoch  19: training Loss: 1195598.5\n",
            "Epoch  20: training Loss: 1088134.625\n",
            "Epoch  21: training Loss: 992130.125\n",
            "Epoch  22: training Loss: 907028.25\n",
            "Epoch  23: training Loss: 832340.375\n",
            "Epoch  24: training Loss: 767473.4375\n",
            "Epoch  25: training Loss: 711620.3125\n",
            "Epoch  26: training Loss: 663749.5\n",
            "Epoch  27: training Loss: 622689.3125\n",
            "Epoch  28: training Loss: 587262.0\n",
            "Epoch  29: training Loss: 556409.75\n",
            "Epoch  30: training Loss: 529274.625\n",
            "Epoch  31: training Loss: 505218.1875\n",
            "Epoch  32: training Loss: 483793.34375\n",
            "Epoch  33: training Loss: 464690.875\n",
            "Epoch  34: training Loss: 447683.46875\n",
            "Epoch  35: training Loss: 432583.0\n",
            "Epoch  36: training Loss: 419212.9375\n",
            "Epoch  37: training Loss: 407394.625\n",
            "Epoch  38: training Loss: 396944.5625\n",
            "Epoch  39: training Loss: 387678.5625\n",
            "Epoch  40: training Loss: 379419.53125\n",
            "Epoch  41: training Loss: 372006.6875\n",
            "Epoch  42: training Loss: 365303.0625\n",
            "Epoch  43: training Loss: 359200.03125\n",
            "Epoch  44: training Loss: 353617.375\n",
            "Epoch  45: training Loss: 348499.0\n",
            "Epoch  46: training Loss: 343807.1875\n",
            "Epoch  47: training Loss: 339515.53125\n",
            "Epoch  48: training Loss: 335602.5\n",
            "Epoch  49: training Loss: 332046.40625\n",
            "Epoch  50: training Loss: 328822.3125\n",
            "Epoch  51: training Loss: 325901.0\n",
            "Epoch  52: training Loss: 323249.96875\n",
            "Epoch  53: training Loss: 320835.1875\n",
            "Epoch  54: training Loss: 318623.8125\n",
            "Epoch  55: training Loss: 316586.25\n",
            "Epoch  56: training Loss: 314697.46875\n",
            "Epoch  57: training Loss: 312938.09375\n",
            "Epoch  58: training Loss: 311293.84375\n",
            "Epoch  59: training Loss: 309754.53125\n",
            "Epoch  60: training Loss: 308312.78125\n",
            "Epoch  61: training Loss: 306962.28125\n",
            "Epoch  62: training Loss: 305696.8125\n",
            "Epoch  63: training Loss: 304509.65625\n",
            "Epoch  64: training Loss: 303393.28125\n",
            "Epoch  65: training Loss: 302339.875\n",
            "Epoch  66: training Loss: 301341.5625\n",
            "Epoch  67: training Loss: 300391.0625\n",
            "Epoch  68: training Loss: 299482.03125\n",
            "Epoch  69: training Loss: 298608.96875\n",
            "Epoch  70: training Loss: 297767.4375\n",
            "Epoch  71: training Loss: 296953.875\n",
            "Epoch  72: training Loss: 296165.4375\n",
            "Epoch  73: training Loss: 295399.6875\n",
            "Epoch  74: training Loss: 294654.8125\n",
            "Epoch  75: training Loss: 293928.9375\n",
            "Epoch  76: training Loss: 293220.4375\n",
            "Epoch  77: training Loss: 292527.71875\n",
            "Epoch  78: training Loss: 291849.1875\n",
            "Epoch  79: training Loss: 291183.5\n",
            "Epoch  80: training Loss: 290529.4375\n",
            "Epoch  81: training Loss: 289886.0625\n",
            "Epoch  82: training Loss: 289252.59375\n",
            "Epoch  83: training Loss: 288628.65625\n",
            "Epoch  84: training Loss: 288013.90625\n",
            "Epoch  85: training Loss: 287408.28125\n",
            "Epoch  86: training Loss: 286811.71875\n",
            "Epoch  87: training Loss: 286224.21875\n",
            "Epoch  88: training Loss: 285645.84375\n",
            "Epoch  89: training Loss: 285076.65625\n",
            "Epoch  90: training Loss: 284516.59375\n",
            "Epoch  91: training Loss: 283965.6875\n",
            "Epoch  92: training Loss: 283423.8125\n",
            "Epoch  93: training Loss: 282890.9375\n",
            "Epoch  94: training Loss: 282366.9375\n",
            "Epoch  95: training Loss: 281851.75\n",
            "Epoch  96: training Loss: 281345.40625\n",
            "Epoch  97: training Loss: 280847.75\n",
            "Epoch  98: training Loss: 280358.71875\n",
            "Epoch  99: training Loss: 279878.125\n",
            "Epoch 100: training Loss: 279405.8125\n",
            "Epoch 101: training Loss: 278941.53125\n",
            "Epoch 102: training Loss: 278484.9375\n",
            "Epoch 103: training Loss: 278035.8125\n",
            "Epoch 104: training Loss: 277593.875\n",
            "Epoch 105: training Loss: 277158.78125\n",
            "Epoch 106: training Loss: 276730.34375\n",
            "Epoch 107: training Loss: 276308.28125\n",
            "Epoch 108: training Loss: 275892.375\n",
            "Epoch 109: training Loss: 275482.40625\n",
            "Epoch 110: training Loss: 275078.1875\n",
            "Epoch 111: training Loss: 274679.5\n",
            "Epoch 112: training Loss: 274286.1875\n",
            "Epoch 113: training Loss: 273898.03125\n",
            "Epoch 114: training Loss: 273514.9375\n",
            "Epoch 115: training Loss: 273136.75\n",
            "Epoch 116: training Loss: 272763.28125\n",
            "Epoch 117: training Loss: 272394.4375\n",
            "Epoch 118: training Loss: 272030.21875\n",
            "Epoch 119: training Loss: 271670.4375\n",
            "Epoch 120: training Loss: 271315.09375\n",
            "Epoch 121: training Loss: 270964.15625\n",
            "Epoch 122: training Loss: 270617.5\n",
            "Epoch 123: training Loss: 270275.15625\n",
            "Epoch 124: training Loss: 269937.03125\n",
            "Epoch 125: training Loss: 269603.09375\n",
            "Epoch 126: training Loss: 269273.3125\n",
            "Epoch 127: training Loss: 268947.625\n",
            "Epoch 128: training Loss: 268626.03125\n",
            "Epoch 129: training Loss: 268308.5\n",
            "Epoch 130: training Loss: 267995.0\n",
            "Epoch 131: training Loss: 267685.5\n",
            "Epoch 132: training Loss: 267380.0\n",
            "Epoch 133: training Loss: 267078.46875\n",
            "Epoch 134: training Loss: 266780.90625\n",
            "Epoch 135: training Loss: 266487.25\n",
            "Epoch 136: training Loss: 266197.375\n",
            "Epoch 137: training Loss: 265911.34375\n",
            "Epoch 138: training Loss: 265629.09375\n",
            "Epoch 139: training Loss: 265350.65625\n",
            "Epoch 140: training Loss: 265075.84375\n",
            "Epoch 141: training Loss: 264804.71875\n",
            "Epoch 142: training Loss: 264537.25\n",
            "Epoch 143: training Loss: 264273.3125\n",
            "Epoch 144: training Loss: 264012.96875\n",
            "Epoch 145: training Loss: 263756.0625\n",
            "Epoch 146: training Loss: 263502.65625\n",
            "Epoch 147: training Loss: 263252.59375\n",
            "Epoch 148: training Loss: 263005.90625\n",
            "Epoch 149: training Loss: 262762.53125\n",
            "Epoch 150: training Loss: 262522.40625\n",
            "Epoch 151: training Loss: 262285.46875\n",
            "Epoch 152: training Loss: 262051.703125\n",
            "Epoch 153: training Loss: 261821.046875\n",
            "Epoch 154: training Loss: 261593.40625\n",
            "Epoch 155: training Loss: 261368.828125\n",
            "Epoch 156: training Loss: 261147.21875\n",
            "Epoch 157: training Loss: 260928.5625\n",
            "Epoch 158: training Loss: 260712.734375\n",
            "Epoch 159: training Loss: 260499.78125\n",
            "Epoch 160: training Loss: 260289.578125\n",
            "Epoch 161: training Loss: 260082.140625\n",
            "Epoch 162: training Loss: 259877.40625\n",
            "Epoch 163: training Loss: 259675.34375\n",
            "Epoch 164: training Loss: 259475.890625\n",
            "Epoch 165: training Loss: 259279.03125\n",
            "Epoch 166: training Loss: 259084.703125\n",
            "Epoch 167: training Loss: 258892.875\n",
            "Epoch 168: training Loss: 258703.53125\n",
            "Epoch 169: training Loss: 258516.59375\n",
            "Epoch 170: training Loss: 258332.046875\n",
            "Epoch 171: training Loss: 258149.875\n",
            "Epoch 172: training Loss: 257970.0\n",
            "Epoch 173: training Loss: 257792.421875\n",
            "Epoch 174: training Loss: 257617.09375\n",
            "Epoch 175: training Loss: 257444.015625\n",
            "Epoch 176: training Loss: 257273.09375\n",
            "Epoch 177: training Loss: 257104.328125\n",
            "Epoch 178: training Loss: 256937.703125\n",
            "Epoch 179: training Loss: 256773.1875\n",
            "Epoch 180: training Loss: 256610.71875\n",
            "Epoch 181: training Loss: 256450.28125\n",
            "Epoch 182: training Loss: 256291.890625\n",
            "Epoch 183: training Loss: 256135.453125\n",
            "Epoch 184: training Loss: 255980.96875\n",
            "Epoch 185: training Loss: 255828.421875\n",
            "Epoch 186: training Loss: 255677.78125\n",
            "Epoch 187: training Loss: 255528.984375\n",
            "Epoch 188: training Loss: 255382.03125\n",
            "Epoch 189: training Loss: 255236.921875\n",
            "Epoch 190: training Loss: 255093.609375\n",
            "Epoch 191: training Loss: 254952.046875\n",
            "Epoch 192: training Loss: 254812.234375\n",
            "Epoch 193: training Loss: 254674.125\n",
            "Epoch 194: training Loss: 254537.71875\n",
            "Epoch 195: training Loss: 254403.0\n",
            "Epoch 196: training Loss: 254269.90625\n",
            "Epoch 197: training Loss: 254138.40625\n",
            "Epoch 198: training Loss: 254008.515625\n",
            "Epoch 199: training Loss: 253880.21875\n",
            "Epoch 200: training Loss: 253753.46875\n",
            "Epoch 201: training Loss: 253628.203125\n",
            "Epoch 202: training Loss: 253504.484375\n",
            "Epoch 203: training Loss: 253382.203125\n",
            "Epoch 204: training Loss: 253261.375\n",
            "Epoch 205: training Loss: 253141.96875\n",
            "Epoch 206: training Loss: 253023.984375\n",
            "Epoch 207: training Loss: 252907.390625\n",
            "Epoch 208: training Loss: 252792.15625\n",
            "Epoch 209: training Loss: 252678.28125\n",
            "Epoch 210: training Loss: 252565.6875\n",
            "Epoch 211: training Loss: 252454.40625\n",
            "Epoch 212: training Loss: 252344.390625\n",
            "Epoch 213: training Loss: 252235.65625\n",
            "Epoch 214: training Loss: 252128.125\n",
            "Epoch 215: training Loss: 252021.84375\n",
            "Epoch 216: training Loss: 251916.734375\n",
            "Epoch 217: training Loss: 251812.78125\n",
            "Epoch 218: training Loss: 251710.03125\n",
            "Epoch 219: training Loss: 251608.375\n",
            "Epoch 220: training Loss: 251507.84375\n",
            "Epoch 221: training Loss: 251408.390625\n",
            "Epoch 222: training Loss: 251310.03125\n",
            "Epoch 223: training Loss: 251212.734375\n",
            "Epoch 224: training Loss: 251116.484375\n",
            "Epoch 225: training Loss: 251021.265625\n",
            "Epoch 226: training Loss: 250927.03125\n",
            "Epoch 227: training Loss: 250833.78125\n",
            "Epoch 228: training Loss: 250741.53125\n",
            "Epoch 229: training Loss: 250650.234375\n",
            "Epoch 230: training Loss: 250559.859375\n",
            "Epoch 231: training Loss: 250470.4375\n",
            "Epoch 232: training Loss: 250381.90625\n",
            "Epoch 233: training Loss: 250294.28125\n",
            "Epoch 234: training Loss: 250207.515625\n",
            "Epoch 235: training Loss: 250121.625\n",
            "Epoch 236: training Loss: 250036.609375\n",
            "Epoch 237: training Loss: 249952.390625\n",
            "Epoch 238: training Loss: 249869.0\n",
            "Epoch 239: training Loss: 249786.4375\n",
            "Epoch 240: training Loss: 249704.6875\n",
            "Epoch 241: training Loss: 249623.6875\n",
            "Epoch 242: training Loss: 249543.46875\n",
            "Epoch 243: training Loss: 249464.03125\n",
            "Epoch 244: training Loss: 249385.328125\n",
            "Epoch 245: training Loss: 249307.359375\n",
            "Epoch 246: training Loss: 249230.109375\n",
            "Epoch 247: training Loss: 249153.5625\n",
            "Epoch 248: training Loss: 249077.71875\n",
            "Epoch 249: training Loss: 249002.578125\n",
            "Epoch 250: training Loss: 248928.125\n",
            "Epoch 251: training Loss: 248854.3125\n",
            "Epoch 252: training Loss: 248781.171875\n",
            "Epoch 253: training Loss: 248708.6875\n",
            "Epoch 254: training Loss: 248636.84375\n",
            "Epoch 255: training Loss: 248565.59375\n",
            "Epoch 256: training Loss: 248495.03125\n",
            "Epoch 257: training Loss: 248425.03125\n",
            "Epoch 258: training Loss: 248355.609375\n",
            "Epoch 259: training Loss: 248286.8125\n",
            "Epoch 260: training Loss: 248218.609375\n",
            "Epoch 261: training Loss: 248150.96875\n",
            "Epoch 262: training Loss: 248083.890625\n",
            "Epoch 263: training Loss: 248017.375\n",
            "Epoch 264: training Loss: 247951.40625\n",
            "Epoch 265: training Loss: 247885.96875\n",
            "Epoch 266: training Loss: 247821.109375\n",
            "Epoch 267: training Loss: 247756.75\n",
            "Epoch 268: training Loss: 247692.890625\n",
            "Epoch 269: training Loss: 247629.5625\n",
            "Epoch 270: training Loss: 247566.71875\n",
            "Epoch 271: training Loss: 247504.390625\n",
            "Epoch 272: training Loss: 247442.578125\n",
            "Epoch 273: training Loss: 247381.1875\n",
            "Epoch 274: training Loss: 247320.34375\n",
            "Epoch 275: training Loss: 247259.953125\n",
            "Epoch 276: training Loss: 247200.015625\n",
            "Epoch 277: training Loss: 247140.515625\n",
            "Epoch 278: training Loss: 247081.484375\n",
            "Epoch 279: training Loss: 247022.90625\n",
            "Epoch 280: training Loss: 246964.765625\n",
            "Epoch 281: training Loss: 246907.078125\n",
            "Epoch 282: training Loss: 246849.765625\n",
            "Epoch 283: training Loss: 246792.921875\n",
            "Epoch 284: training Loss: 246736.484375\n",
            "Epoch 285: training Loss: 246680.46875\n",
            "Epoch 286: training Loss: 246624.84375\n",
            "Epoch 287: training Loss: 246569.609375\n",
            "Epoch 288: training Loss: 246514.796875\n",
            "Epoch 289: training Loss: 246460.359375\n",
            "Epoch 290: training Loss: 246406.328125\n",
            "Epoch 291: training Loss: 246352.65625\n",
            "Epoch 292: training Loss: 246299.359375\n",
            "Epoch 293: training Loss: 246246.453125\n",
            "Epoch 294: training Loss: 246193.890625\n",
            "Epoch 295: training Loss: 246141.6875\n",
            "Epoch 296: training Loss: 246089.84375\n",
            "Epoch 297: training Loss: 246038.34375\n",
            "Epoch 298: training Loss: 245987.234375\n",
            "Epoch 299: training Loss: 245936.4375\n"
          ]
        }
      ],
      "source": [
        "# TEST\n",
        "losses, network = train_glove_vectors(vocab, X_weighted, X1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roq7Ubklgel4"
      },
      "source": [
        "<font color=\"red\">**Don't clear the output of the above cell!**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n6RCKKbgel4",
        "nbgrader": {
          "grade": false,
          "grade_id": "len_losses",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c6.1\n",
        "assert len(losses) == 300\n",
        "assert losses[0] > losses[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3iCGDjDgel4"
      },
      "source": [
        "Plot the losses (x axis for epoch number and y axis for loss) and examine the learning curve. Ask yourself, is its shape what you would expect it to be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emLdJBO_gel4",
        "nbgrader": {
          "grade": true,
          "grade_id": "plot",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "cd52fec8-3950-4cff-cabf-3faa7109d5d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title c6.2 [0pt]\n",
        "# YOUR CODE HERE\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pM9LESdgel4"
      },
      "source": [
        "<font color=\"red\">**Don't clear the output of the above cell!**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqu-EFqGgel4"
      },
      "source": [
        "### Validation (Similarity)\n",
        "\n",
        "Curious to see what this network has learned? Let's perform a simple validation experiment.\n",
        "\n",
        "We will check which words the model considers the most similar to other words. To that end, we need a notion of __similarity__. One of the most common measures of similarity in high-dimensional vector spaces is the cosine similarity.\n",
        "\n",
        "The cosine similarity of two vectors $\\vec{a}, \\vec{b}$ is given as:\n",
        "$$sim(\\vec{a}, \\vec{b}) = \\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}|_2 \\cdot |\\vec{b}|_2}$$\n",
        "\n",
        "where $|\\vec{x}|_2$ is the $L_2$-norm of the $\\vec{x}$.\n",
        "\n",
        "The function `similarity` below accepts two words, a vocabulary and the network's output vectors, and computes the similarity between these two words. For an outside-vocabulary word similarity is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnFdO_3wgel5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ab85deea538994af",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def similarity(word_i: str, word_j: str, vocab: Dict[str, int], vectors: FloatTensor) -> float:\n",
        "    if not(word_i in vocab and word_i in vocab): return 0.\n",
        "    i, j = vocab[word_i], vocab[word_j]\n",
        "    v_i = vectors[i] / torch.linalg.vector_norm(vectors[i])  # a/|a|\n",
        "    v_j = vectors[j] / torch.linalg.vector_norm(vectors[j])  # b/|b|\n",
        "    sim = torch.dot(v_i, v_j)\n",
        "    return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REXzC6nzgel5"
      },
      "source": [
        "Let's check out some examples. Consider the word pairs below and, optionally, add your own word pairs if it helps to support your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ8W10bOgel5",
        "nbgrader": {
          "grade": true,
          "grade_id": "similarity_sandbox",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2412e05c-d4c0-4c71-cde0-07fa6ee6de2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'cruciatus' and 'imperius' is: 0.4987607002258301\n",
            "Similarity between 'avada' and 'kedavra' is: 0.6552069187164307\n",
            "Similarity between 'hogwarts' and 'school' is: 0.7654938697814941\n",
            "Similarity between 'goblin' and 'hagrid' is: 0.27460339665412903\n",
            "Similarity between 'giant' and 'hagrid' is: 0.5027310848236084\n"
          ]
        }
      ],
      "source": [
        "word_vectors = network.get_vectors().detach()\n",
        "\n",
        "for pair in [\n",
        "    (\"cruciatus\", \"imperius\"),\n",
        "    (\"avada\", \"kedavra\"),\n",
        "    (\"hogwarts\", \"school\"),\n",
        "    (\"goblin\", \"hagrid\"),\n",
        "    (\"giant\", \"hagrid\"),\n",
        "]:\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' is: {similarity(pair[0], pair[1], vocab, word_vectors)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oh6nme_gel5"
      },
      "source": [
        "#### i3 [2pt]\n",
        "Give an interpretation of the results. Do the scores correspond well to your perceived similarity of these word pairs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJjjY4PEgel5",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation3",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "Small note: Both of us have not read Harry Potter, so these interpretations were sometimes a bit challenging for us. We tried our best using the provided Harry Potter Wiki.\n",
        "\n",
        "Cruciatus and Imperius have a high similarity score. This is not surprising to us, as both of these are types of unforgivable curses.\n",
        "\n",
        "Avada and Kedavra have an even higher similarity score. This is also not that surprising, as a combination of them forms one curse: ‘avada kedavra’. We expect them to be used in similar contexts, and also together.\n",
        "\n",
        "Hogwarts and School also have a very high similarity. This meets our expectations, as Hogwarts is the School in the Harry Potter world, and we expect characters to refer to these words interchangeably and in similar contexts.\n",
        "\n",
        "Goblin and Hagrid have a relatively low similarity. We expect this could be because a goblin is a type of character, and Hagrid is one specific character.\n",
        "\n",
        "Giant and Hagrid have a very low similarity. This quite surprised us: while a giant is again a type of character, while Hagrid is a character, Hagrid is also half giant. Therefore, we expected this similarity to be a bit higher.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyaWr70ygel5"
      },
      "source": [
        "To obtain the similarities of one word against all other words in the corpus, use torch functions and follow the equation ($\\vec{c}_i$ is $i$th row of $\\mathbf{C}$):\n",
        "$$sim(\\vec{w}, \\mathbf{C}) = \\frac{\\vec{w}\\cdot \\mathbf{C}}{|\\vec{w}|_2 \\cdot |\\mathbf{C}|_2} = \\Big(\\frac{\\vec{w}\\cdot \\vec{c}_1}{|\\vec{w}|_2 \\cdot |\\vec{c}_1|_2},\\ldots,\\frac{\\vec{w}\\cdot \\vec{c}_N}{|\\vec{w}|_2 \\cdot |\\vec{c}_N|_2}\\Big)$$\n",
        "\n",
        "Using `similarity` as a reference, write `similarities`, which accepts one word, a vocabulary and the network's output vectors and computes the similarity between the word and the entire corpus. **If a word is out of vocabulary, it should return a matrix of 0 similarities.**\n",
        "\n",
        "_Hint_: $\\mathbf{C} \\in \\mathbb{R}^{N, D}$, $\\vec{w} \\in \\mathbb{R}^{1, D}$, $sim(\\vec{w}, \\mathbf{C}) \\in \\mathbb{R}^{1, N}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_9TvkOBgel5",
        "nbgrader": {
          "grade": true,
          "grade_id": "similarities",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c7 [4pt]\n",
        "def similarities(word_i: str, vocab: Dict[str, int], vectors: FloatTensor) -> FloatTensor:\n",
        "    if not(word_i in vocab):\n",
        "        return torch.zeros(1, vectors.size(0), dtype=vectors.dtype, device=vectors.device)\n",
        "    i = vocab[word_i]\n",
        "    vector_i = vectors[i].unsqueeze(0)\n",
        "    sim = torch.matmul(vector_i, vectors.T)\n",
        "    return sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv37utYPZ3nQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef01ca6d-6948-4bc8-b20e-ae8518d6c6f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "torch.tensor([1, 2, 3, 4]).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctJObbzAgel5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-74c0c2f09dccce6c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c7a\n",
        "assert similarities('harry', vocab, word_vectors).shape == torch.Size([1, len(vocab)])\n",
        "assert similarities('cow', vocab, word_vectors).shape == torch.Size([1, len(vocab)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7AaywkIgel6"
      },
      "source": [
        "Now we can manipulate the word vectors to find out what the corpus-wide most similar words to a query word are!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTThuWx1gel6",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b702f18eb2920639",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def most_similar(word_i: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
        "    \"\"\" Returns a list of k words that are most similar to word_i\n",
        "        The list excludes word_i itself\n",
        "    \"\"\"\n",
        "    sims = similarities(word_i, vocab, vectors)\n",
        "    _, topi = sims.topk(dim=-1, k=k+1)\n",
        "    topi = topi.view(-1).cpu().numpy().tolist()\n",
        "    inv = {v: i for i, v in vocab.items()}\n",
        "    return [inv[i] for i in topi if inv[i] != word_i][:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exbym9XKnsPY",
        "outputId": "f079c551-ad79-4a23-d91b-b7007ce4efb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'forbidden': ['cabin', 'forest', 'lawns', 'grounds', 'deeper', 'fer']\n",
            "Most similar words to 'myrtle': ['bathroom', 'moaning', 'toilet', 'nigellus', 'krum', 'ernie']\n",
            "Most similar words to 'gryffindor': ['points', 'ravenclaw', 'slytherin', 'hufflepuff', 'wood', 'team']\n",
            "Most similar words to 'wand': ['could', 'harry', 'face', 'light', 'well', 'head']\n",
            "Most similar words to 'quidditch': ['team', 'training', 'tournament', 'match', 'seeker', 'oliver']\n",
            "Most similar words to 'marauder': ['map', 'lady', 'tower', 'eat', 'defense', 'dormitory']\n",
            "Most similar words to 'horcrux': ['sword', 'goblin', 'locket', 'horcruxes', 'ring', 'griphook']\n",
            "Most similar words to 'phoenix': ['feather', 'fawkes', 'minister', 'wand', 'ollivander', 'exist']\n",
            "Most similar words to 'triwizard': ['tournament', 'champions', 'cedric', 'champion', 'task', 'quidditch']\n",
            "Most similar words to 'screaming': ['scream', 'expecto', 'voices', 'footsteps', 'jet', 'screams']\n",
            "Most similar words to 'letter': ['hedwig', 'errol', 'ink', 'owl', 'envelope', 'pigwidgeon']\n"
          ]
        }
      ],
      "source": [
        "# TEST c7b\n",
        "for word in [\n",
        "    \"forbidden\", \"myrtle\", \"gryffindor\", \"wand\", \"quidditch\", \"marauder\",\n",
        "    \"horcrux\", \"phoenix\", \"triwizard\", \"screaming\", \"letter\"\n",
        "]:\n",
        "    print(f\"Most similar words to '{word}': {most_similar(word, vocab, word_vectors, 6)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXkuHiogel6"
      },
      "source": [
        "#### i4 [3pt]\n",
        "\n",
        "Interpret the results in the context of Harry Potter books.\n",
        "- Do these most similar words make sense (are they actually similar to the query words)?\n",
        "- Are there any patterns you can see in the \"errors\" (the words that you wouldn't consider actually similar to the query word in general, in everyday life)?\n",
        "- Which examples are instances of similarity (if any) and relatedness (if any)?\n",
        "- Any other observations are welcome.\n",
        "\n",
        "Illustrate your answers with examples from your model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBR7DEz_gel7",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation4",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "Many of these similar words make sense, which gives us some trust in the working of our programme. A clear example of this is the results of the query word ‘letter’, for which similar words are found like ‘hedwig’ (the owl that often delivers letters to Harry), ‘errol’ (the Weaseley family’s owl), and ‘ink’ and ‘enveloppe’ (items closely related to a letter). Another example of this is quidditch (a sport in the Harry Potter world), which shows similarities to sports related terms like ‘team’, ‘training’, ‘tournament’ and ‘match’.\n",
        "\n",
        "We have also noticed a pattern in what seem to be errors (words that are actually not that similar). We suspect these might often not be the result of the words being similar, but of them often being used together. For example: the query ‘wand’ results in words like ‘could’ and ‘face’. While these words are not similar, we can see these words being used together quite often. As wands are often mentioned in Harry Potter, we expect wands to be, for example “often waved in front of his face” of “the wand lighting his face quite suddenly”.\n",
        "\n",
        "For the words that are in fact, similar, we can differentiate between two kinds of instances: those of similarity and relatedness. Similarity is visible with the query ‘gryffindor’, where it results in the other houses at Hogwarts: ‘ravenclaw’, ‘slytherin’ and ‘hufflepuff’. An example of relatedness is seen at the query ‘forbidden’. This results in ‘forest’, which does not directly show a semantic similarity to the query, but it is closely related to it, as both words combined form the name of a place: the Forbidden Forest.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdp1FSNbgel8"
      },
      "source": [
        "Overall it's quite impressive; we managed to encode a meaningful portion of the corpus statistics in only $30$ numbers per word!\n",
        "(A compression ratio of >99%)\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b> The word vectors obtained by this process are (to a small extent) random, due to the random initialization of the embedding layers. If you are unhappy with your results, you can repeat the experiment a few times or try to toy around with the hyper-parameters (the smoothing factor of ${X}$, $x_{max}$, $\\alpha$, the number of epochs and the dimensionality of the vector space). But do this as your own experiments outside the submission file.\n",
        "</div>\n",
        "\n",
        "Word vectors, however, contain way more information than just word co-occurrence statistics. Hold tight until the next assignment, where we will see how word vectors may be used to infer information spanning entire phrases and sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eypgbwmMgel8"
      },
      "source": [
        "### Validation (Word Analogies)\n",
        "\n",
        "From the paper:\n",
        "> The word analogy task consists of questions like \"$a$ is to $b$ as $c$ is to __?\" To correctly answer this question, we must find the word $d$ such that $w_d \\approx w_b - w_a + w_c$ according to the cosine similarity.\n",
        "\n",
        "Write your own function that performs the word analogy task.  \n",
        "In addition to the standard approximation $w_d \\approx w_b - w_a + w_c$, implement another non-standard but intuitive approximation where we are searching $w_d$ that maximizes teh following similarity $w_a - w_b \\approx w_c - w_d$.  \n",
        "Let's see which criteria will give the best results, or maybe both give the same results.    \n",
        "\n",
        "_Hint_: Take a look at the code a few cells back. Most of what you need is already there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDNEXZL2gel8",
        "nbgrader": {
          "grade": true,
          "grade_id": "analogy",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Dict\n",
        "\n",
        "def analogy(\n",
        "    word_a: str, word_b: str, word_c: str, vocab: Dict[str, int],\n",
        "    vectors: torch.FloatTensor, k: int = 5, standard: bool = True\n",
        ") -> List[str]:\n",
        "    \"\"\"Return a list of k words whose vectors are most similar to the solution vector of the analogy.\n",
        "    The first element means the most similar, followed by the rest in decreasing similarity.\n",
        "    word_a, word_b, and word_c are never returned as a part of the list.\n",
        "\n",
        "    param standard=True uses the standard approximation equation, False uses the alternative one.\n",
        "    \"\"\"\n",
        "    if not (word_a in vocab and word_b in vocab and word_c in vocab):\n",
        "        return []\n",
        "\n",
        "    # Get the indices of the words\n",
        "    a = vocab[word_a]\n",
        "    b = vocab[word_b]\n",
        "    c = vocab[word_c]\n",
        "\n",
        "    if standard:\n",
        "        # Standard analogy: w_d ≈ w_b - w_a + w_c\n",
        "        vec_d = vectors[b] - vectors[a] + vectors[c]\n",
        "        vec_d = vec_d / torch.linalg.vector_norm(vec_d)\n",
        "\n",
        "        vectors_normalized = vectors / torch.linalg.vector_norm(vectors, dim=1, keepdim=True)\n",
        "        similarities = torch.matmul(vec_d.unsqueeze(0), vectors_normalized.T).squeeze()\n",
        "    else:\n",
        "        # Non-standard analogy: w_a - w_b ≈ w_c - w_d\n",
        "        analogy_vector = vectors[a] - vectors[b]\n",
        "        analogy_vector = analogy_vector / torch.linalg.vector_norm(analogy_vector)\n",
        "\n",
        "        vector_diff_cd = vectors[c] - vectors  # (w_c - w_d) for all d in one step\n",
        "        vector_diff_cd = vector_diff_cd / torch.linalg.vector_norm(vector_diff_cd, dim=1, keepdim=True)\n",
        "\n",
        "        # Compute the cosine similarity using matrix multiplication\n",
        "        similarities = torch.matmul(analogy_vector.unsqueeze(0), vector_diff_cd.T).squeeze()\n",
        "\n",
        "    # Get top k most similar words, excluding word_a, word_b, and word_c\n",
        "    _, top_indices = similarities.topk(k + 3)\n",
        "    inv_vocab = {v: i for i, v in vocab.items()}\n",
        "\n",
        "    # Filter out word_a, word_b, and word_c from the results\n",
        "    results = []\n",
        "    for idx in top_indices.cpu().numpy().tolist():\n",
        "        word = inv_vocab.get(idx)\n",
        "        if word not in {word_a, word_b, word_c}:\n",
        "            results.append(word)\n",
        "        if len(results) == k:\n",
        "            break\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title c8 [6pt]\n",
        "def analogy(\n",
        "    word_a: str, word_b: str, word_c: str, vocab: Dict[str, int],\n",
        "    vectors: FloatTensor, k: int=5, standard: bool=True\n",
        ") -> List[str]:\n",
        "    \"\"\" Return a list of k words whose vectors are most similar to the solution vector of the analogy.\n",
        "        The first element means the most similar, followed by the rest in decreasing similarity.\n",
        "        word_a, word_b, and word_c are never returned as a part of the list.\n",
        "        param standard=True uses the standard approximation equation, =False uses the alternative one.\n",
        "    \"\"\"\n",
        "\n",
        "    va = vectors[vocab[word_a]]\n",
        "    vb = vectors[vocab[word_b]]\n",
        "    vc = vectors[vocab[word_c]]\n",
        "\n",
        "    if standard:\n",
        "        sol_vector = vb - va + vc\n",
        "\n",
        "    else:\n",
        "        sol_vector = vc - (va - vb)\n",
        "\n",
        "    sol_vector = sol_vector / torch.linalg.vector_norm(sol_vector)\n",
        "\n",
        "    normalised_all_vectors = vectors / torch.linalg.vector_norm(vectors,dim=1,keepdim=True)\n",
        "    similars = torch.mm(normalised_all_vectors, sol_vector.unsqueeze(1)).squeeze()\n",
        "    _, topi = similars.topk(dim=-1, k=k+3)\n",
        "    topi = topi.view(-1).cpu().numpy().tolist()\n",
        "    inv = {v: i for i, v in vocab.items()}\n",
        "    return [inv[i] for i in topi if inv[i] not in {word_a,word_b,word_c}][:k]\n"
      ],
      "metadata": {
        "id": "cCxWoy-s6Dvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m9PcG8jgel8"
      },
      "source": [
        "Some example triplets to test your analogies on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFkCHKT_gel8",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3f0e833f60eafb32",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4223e5a0-0aae-4165-e9d9-93d571ae6f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: d ~ c - a + b\n",
            "     padma       is to     parvati      as       fred       is to\t['pretending', 'george', 'mrs', 'told', 'ron']\n",
            "     avada       is to     kedavra      as     expecto      is to\t['patronum', 'slowed', 'dementor', 'hood', 'steam']\n",
            "    dungeon      is to    slytherin     as      tower       is to\t['gryffindor', 'training', 'points', 'hundred', 'ravenclaw']\n",
            "    scabbers     is to       ron        as      hedwig      is to\t['carrying', 'awake', 'parcel', 'time', 'five']\n",
            "      ron        is to      molly       as      draco       is to\t['coolly', 'narcissa', 'unaware', 'forgive', 'pleasantly']\n",
            "   durmstrang    is to      viktor      as   beauxbatons    is to\t['delacour', 'maxime', 'cedric', 'judges', 'support']\n",
            "     snape       is to     potions      as    trelawney     is to\t['herbology', 'divination', 'exams', 'exam', 'binns']\n",
            "     harry       is to      seeker      as       ron        is to\t['score', 'chasers', 'hufflepuff', 'flint', 'spectacular']\n",
            "     draco       is to    slytherin     as      harry       is to\t['ron', 'done', 'remember', 'fall', 'rest']\n",
            "     harry       is to      potter      as       ron        is to\t['actually', 'smiling', 'sneered', 'coolly', 'laughed']\n",
            "     harry       is to     hogwarts     as      hagrid      is to\t['committee', 'fer', 'yer', 'indignation', 'jus']\n",
            "\n",
            "Mode: a - b ~ c - d\n",
            "     padma       is to     parvati      as       fred       is to\t['one', 'better', 'moody', 'professor', 'growled']\n",
            "     avada       is to     kedavra      as     expecto      is to\t['accidents', 'whenever', 'fragment', 'gnarled', 'unexpected']\n",
            "    dungeon      is to    slytherin     as      tower       is to\t['points', 'gryffindor', 'win', 'ravenclaw', 'training']\n",
            "    scabbers     is to       ron        as      hedwig      is to\t['cho', 'applause', 'points', 'time', 'cup']\n",
            "      ron        is to      molly       as      draco       is to\t['ere', 'fudge', 'remus', 'kingsley', 'bode']\n",
            "   durmstrang    is to      viktor      as   beauxbatons    is to\t['delacour', 'cedric', 'brightly', 'insides', 'krum']\n",
            "     snape       is to     potions      as    trelawney     is to\t['lunch', 'herbology', 'divination', 'exams', 'chapter']\n",
            "     harry       is to      seeker      as       ron        is to\t['score', 'spectacular', 'flint', 'ireland', 'stands']\n",
            "     draco       is to    slytherin     as      harry       is to\t['points', 'johnson', 'gryffindor', 'hundred', 'oliver']\n",
            "     harry       is to      potter      as       ron        is to\t['coolly', 'actually', 'sneered', 'glare', 'smiling']\n",
            "     harry       is to     hogwarts     as      hagrid      is to\t['committee', 'witchcraft', 'accidents', 'fears', 'indignation']\n"
          ]
        }
      ],
      "source": [
        "# TEST c8\n",
        "triplets = [(\"padma\", \"parvati\", \"fred\"),\n",
        "            (\"avada\", \"kedavra\", \"expecto\"),\n",
        "            (\"dungeon\", \"slytherin\", \"tower\"),\n",
        "            (\"scabbers\", \"ron\", \"hedwig\"),\n",
        "            (\"ron\", \"molly\", \"draco\"),\n",
        "            (\"durmstrang\", \"viktor\", \"beauxbatons\"),\n",
        "            (\"snape\", \"potions\", \"trelawney\"),\n",
        "            (\"harry\", \"seeker\", \"ron\"),\n",
        "            # ---------------------\n",
        "            # YOUR CODE HERE (3 examples)\n",
        "            (\"draco\", \"slytherin\", \"harry\"),\n",
        "            (\"harry\", \"potter\" , \"ron\"),\n",
        "            (\"harry\", \"hogwarts\", \"hagrid\")\n",
        "           ]\n",
        "\n",
        "print(\"Mode: d ~ c - a + b\")\n",
        "for a, b, c in triplets:\n",
        "    print(\"{:^16} is to {:^16} as {:^16} is to\\t{}\".format(a, b, c, analogy(a, b, c, vocab, word_vectors, 5, standard=True)))\n",
        "\n",
        "print(\"\\nMode: a - b ~ c - d\")\n",
        "for a, b, c in triplets:\n",
        "    print(\"{:^16} is to {:^16} as {:^16} is to\\t{}\".format(a, b, c, analogy(a, b, c, vocab, word_vectors, 5, standard=False)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOUQSblcgel9"
      },
      "source": [
        "Some minimal emergent intelligence :) *(hopefully..)*. 🧙‍♀️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_V5ITEpgel9"
      },
      "source": [
        "#### i5 [3pt]\n",
        "\n",
        "Which analogy equation did give the best results for the provided analogy samples?  \n",
        "\n",
        "Come up with three additional analogies in the context of Harry Potter.\n",
        "Add them to the list of analogies in the code above and run the analogy computation.\n",
        "\n",
        "Interpret the results of the best performing analogy equation (if any):\n",
        "- For which analogies did the model manage to guess the correct answers (taking the first word in the output to be the model's \"guess\")?\n",
        "- For which analogies are the correct answers present in the top K words?\n",
        "- Do you see any patterns in the cases when the model didn't solve the task correctly? In other words, when the model's guess was wrong, can you suggest why the model guessed what it guessed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKvYAag0gel9",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation5",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "We noticed that the standard equation produced better results than the non-standard equation. The top k words seem to be more related to the word to which an allegory is being approximated.\n",
        "\n",
        "We see one clearly correct answer in “draco is to slytherin as harry is to [..., gryffindor, ...]”. While it is the third word, this does indeed embody the correct relationship (Harry’s Hogwarts house). An (almost) correct match is visible in “harry is to seeker as ron is to [..., chasers, ...]”. While it is the a role in Quidditch games, it is not the one that Ron holds.\n",
        "\n",
        "A pattern that we see in the incorrect words, is that the model often assigns general words, which are not relevant to the analogy tasks. It often results in words that are often used in dialogues and their descriptions ‘smiling’, ‘sneered’, or ‘gnarled’; We understand that these words, because of there frequent usages, might show up in these top k words, but these are not the kind of words we are looking for in this analogy task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ne6B-pqDcW"
      },
      "source": [
        "## Counting co-occurrences\n",
        "\n",
        "In the beginning of the notebook you were provided with a pickle of co-occurrence counts. But how were these counts obtained? You will find this out when you complete this exercise.  \n",
        "\n",
        "First, we obtain the plain text corpus of the Harry Potter book series and place the book files in the current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uXyAJQ3Zvffu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f909ce-7316-4126-c4e5-4d9d5f73cd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Harry-Potter-Text-Mining'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 106 (from 1)\u001b[K\n",
            "Receiving objects: 100% (109/109), 13.39 MiB | 24.35 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4GHUKmaywCYt"
      },
      "outputs": [],
      "source": [
        "!cp Harry-Potter-Text-Mining/Book\\ Text/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QordGEyDXE35"
      },
      "source": [
        "In research, it is relatively common that papers don't include all necessary details for replicating the experiments. This is often due to the lack of space in the paper, overlooking certain details, or simply a bad practice.\n",
        "\n",
        "We will give you several hints on how the co-occurrence matrix was obtained. With the hints and the provided data in the pickle file, you should be able to replicate the exact content of the co-occurrence matrix from the pickle. Here are the hints:\n",
        "\n",
        "1.   Tokenization of the data was done in a shallow but effective way: replace all punctuations with white space and delimit tokens with a sequence of white spaces;\n",
        "2.   Standard libraries (!) were used for identifying punctuations and stopwords;\n",
        "3.   Context windows are symmetric and don't span across chapters (i.e., the last word of chapter $N$ is not co-occuring with the first word of chapter $N+1$).\n",
        "4.   The vocabulary was obtained based on the frequency cutoff.\n",
        "\n",
        "To replicate the co-occurrence counts, complete the function below. Then use it to find out the exact values of `cutoff` and `window_size` that result in the co-occurrence counts identical to the provided ones. No need to provide the code for finding the exact values. Just use them in the next TEST code cell.\n",
        "\n",
        "Feel free to import any standard python library. Use the same cell code for importing libraries and defining auxiliary functions (if any).\n",
        "The code will be evaluated not only on the correctness of the output but also on efficiency. It should be able to extract the counts and create the tensor in less than a minute (in the colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj7YgTR-uwFW",
        "outputId": "6195d7c8-177b-41f3-8f4d-41ebafb1497b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qwE8faHMwqwU"
      },
      "outputs": [],
      "source": [
        "# @title c9 [12pt]\n",
        "\n",
        "import torch\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\" Tokenizes text by replacing punctuations with spaces, and splits by whitespace \"\"\"\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # Replace punctuation with spaces\n",
        "    tokens = text.lower().split()  # Tokenize by whitespace\n",
        "    tokens = [token for token in tokens if token not in nltk_stopwords]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "def read_book_files(files: List[str], cutoff: int=10, window_size: int=20, verbose: bool=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        files - a list of file paths;\n",
        "        cutoff - vocab excludes words with occurrence counts < cutoff;\n",
        "        window_size - size of the left and right windows (individually);\n",
        "        verbose - when set to False prints nothing.\n",
        "    Outputs a tuple of:\n",
        "        voc - mapping of vocabulary to indices;\n",
        "        dd_cnt - dict of dict of co-occurrence counts;\n",
        "        X - Tensor with |vocab|x|vocab| size and co-occurrence counts.\n",
        "    \"\"\"\n",
        "    word_counts = Counter()\n",
        "    co_occurrence_counts = defaultdict(lambda: defaultdict(int))\n",
        "    all_tokens = []\n",
        "    for file in files:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            tokens = tokenize(text)\n",
        "            all_tokens.append(tokens)\n",
        "\n",
        "    all_tokens = [token for chapter in all_tokens for token in chapter]\n",
        "\n",
        "    word_counts.update(all_tokens)\n",
        "\n",
        "    vocab = {word for word, count in word_counts.items() if count >= cutoff}\n",
        "    vocab = sorted(vocab)  # Sort for consistency\n",
        "    voc = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    for i, token in enumerate(all_tokens):\n",
        "        if token not in vocab:\n",
        "            continue\n",
        "        for j in range(max(0, i - window_size), min(len(all_tokens), i + window_size + 1)):\n",
        "            if i != j and all_tokens[j] in vocab:\n",
        "                co_occurrence_counts[token][all_tokens[j]] += 1\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "    X = torch.zeros((vocab_size, vocab_size), dtype=torch.float32)\n",
        "\n",
        "    for word_i, neighbors in co_occurrence_counts.items():\n",
        "        idx_i = voc[word_i]\n",
        "        for word_j, count in neighbors.items():\n",
        "            idx_j = voc[word_j]\n",
        "            X[idx_i, idx_j] = count\n",
        "    if verbose:\n",
        "        print(f'Vocabulary size: {vocab_size}')\n",
        "\n",
        "    return voc, co_occurrence_counts, X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qWtcGMkLpvY5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "148da276-5904-48b5-8310-3efd0048e4a7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-838d542f7496>\u001b[0m in \u001b[0;36mread_book_files\u001b[0;34m(files, cutoff, window_size, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title c9 [12pt]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TEST\n",
        "# YOUR CODE HERE - Fill in the found cutoff and window_size values\n",
        "# it shouldn't take more than 2min\n",
        "# ours does not run, sadly... sorry for that! :( we tried our best with the previous exercise\n",
        "%%time\n",
        "my_vocab, my_contexts, my_X = read_book_files([ f\"/content/HPBook{n}.txt\" for n in range(1,8) ], cutoff=22, window_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etLoCWEw5ay7"
      },
      "outputs": [],
      "source": [
        "# TEST c9\n",
        "# This makes sure the value types are as expected\n",
        "assert my_vocab['1'] == 0\n",
        "assert isinstance(my_vocab, dict)\n",
        "assert isinstance(my_contexts, dict)\n",
        "assert 'potter' in my_contexts['harry']\n",
        "assert isinstance(my_X, torch.Tensor)\n",
        "assert my_X.size() == (4298,4298)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t29uPQsQhBnr"
      },
      "source": [
        "<font color=\"red\">**Don't clear the output of the above cell!**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwjIMegPnE7_"
      },
      "source": [
        "#### i6 [3pt]\n",
        "\n",
        "What are the values of `cutoff` and `window_size` that replicates the provided counts?\n",
        "\n",
        "Briefly describe the process of finding the correct underlying algorithm of `read_book_files` and the correct values of `cutoff` and `window_size`.  \n",
        "Be concise. 100-150 words can be more than enough for the description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIslxPDHnBWh",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation5",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "In order to solve this, we aimed to start by tokenizing the text, replacing all punctuation with whitespace, to ensure tokens are separated by sequences of whitespace. To do this, we rely on standard libraries such as string for punctuation and nltk (library) for stopwords. After this, we constructed the vocabulary by applying a frequency-cutoff, including only words that appear frequently enough. Finally, we create the co-occurrence matrix using a symmetric context window around each word, making sure that words from different chapters do not co-occur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h2l-Sw7gel9"
      },
      "source": [
        "### Optional\n",
        "If you are done, you can continue experimenting in order to understand the system's behaviour better. For example: how does training and hyperparameter choice affect the model's performance?\n",
        "Repeat the training using your own hyperparameters (vector space dimensionality, optimizer parameters, the number of training epochs, a different random seed, etc.).\n",
        "\n",
        "During the training loop, print the qualitative benchmarks every few epochs. Do they keep improving? Is there any disadvantage to exhaustively training until convergence?\n",
        "\n",
        "Now you have all the tools at hand to train the word vectors on any textual corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbgNN_S8W9Y6"
      },
      "source": [
        "# Acknowledgment\n",
        "\n",
        "The jupyter notebook was initially created by Konstantinos Kogkalidis and Tejaswini Deoskar.  \n",
        "Recent changes, including adaptation to the Colab environment and the exercise on reconstructing a co-occurrence matrix, are by Lasha Abzianidze."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "toc-showmarkdowntxt": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}